{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "# Seed to replicate the splitting process. Do not change it!.\n",
    "SEED = 42\n",
    "random.seed(42)\n",
    "\n",
    "stop_word_java_tag = [#'@author',\n",
    "                      '@param',\n",
    "                      '@deprecated',\n",
    "                      '@return',\n",
    "                      #'@see',\n",
    "                      '{@link}',\n",
    "                      #'@since',\n",
    "                      '@throws',\n",
    "                      '@Override',\n",
    "                      '{@docRoot}',\n",
    "                      '@exception',\n",
    "                      '{@inheritDoc}',\n",
    "                      '{@linkplain}',\n",
    "                      '{@literal}',\n",
    "                      '@serial',\n",
    "                      '@serialData',\n",
    "                      #'@version',\n",
    "                      '@{value}',\n",
    "                      '@argfiles'\n",
    "                ]\n",
    "\n",
    "###### PLACEHOLDERS ######\n",
    "PLACEHOLDER_MASK = '<extra_id_0>'\n",
    "FAKE_LINK = '|__link__|'\n",
    "FAKE_REF =  '|__ref__|'\n",
    "#########################\n",
    "\n",
    "TOKEN_LEN = 256\n",
    "\n",
    "###### T5 SETTINGS ######\n",
    "T5_EOS = ' </s>'\n",
    "JAVADOC_PREFIX = 'complete javadoc comment: '\n",
    "BLOCK_COMMENT_PREFIX =  'complete block/inline comment: '\n",
    "#########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def splitJavaDocByTag(javadoc):\n",
    "\n",
    "    javadoc_sentences = []\n",
    "    items = javadoc.split()\n",
    "    delimiters = [word for word in items if word.startswith(\"@\") and word in stop_word_java_tag]\n",
    "    for idx in range(0, len(delimiters)):\n",
    "\n",
    "        if idx==len(delimiters)-1:\n",
    "            starting_index = javadoc.rfind(delimiters[idx])\n",
    "            st = javadoc[starting_index:].strip('<sep>')\n",
    "            javadoc_sentences.append(st)\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            sentence = re.findall(\"%s(.*?)%s\" % (delimiters[idx],delimiters[idx+1]), javadoc)[0]\n",
    "            javadoc_sentences.append(delimiters[idx]+sentence)\n",
    "            #print('--> %s%s' % (delimiters[idx], sentence))\n",
    "\n",
    "        javadoc = javadoc.replace(delimiters[idx],'', 1)\n",
    "\n",
    "    return javadoc_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Quick fix\n",
    "def check4Context(item):\n",
    "\n",
    "    lines = item.splitlines()\n",
    "\n",
    "    for (id_line, line) in enumerate(lines):\n",
    "\n",
    "        if line.strip().startswith('<sep>'):\n",
    "\n",
    "            if id_line >= 1:\n",
    "\n",
    "                if len(lines[id_line-1])==0:\n",
    "                    #we have to tight the context\n",
    "                    for i in range(0, id_line):\n",
    "                        lines[i]='||_to_remove_||'\n",
    "                    break\n",
    "\n",
    "    refined_item = ''\n",
    "    for line in lines:\n",
    "        if '||_to_remove_||' in line:\n",
    "            continue\n",
    "        else:\n",
    "            refined_item+=line\n",
    "\n",
    "    if refined_item.startswith('<sep>') and refined_item.endswith('<sep>'):\n",
    "        return None\n",
    "    else:\n",
    "        return refined_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load finetuning dataset\n",
    "\n",
    "\n",
    "with open('finetuning_single_comment.pickle', 'rb') as newObj:\n",
    "    data_finetune = pickle.load(newObj)\n",
    "    ft_not_keep_comment = []\n",
    "    for item in data_finetune:\n",
    "        refined = check4Context(item)\n",
    "        if refined is not None:\n",
    "            ft_not_keep_comment.append(refined)\n",
    "\n",
    "\n",
    "ft_javadoc_list = []\n",
    "with open('finetuning_javadoc.pickle', 'rb') as newObj:\n",
    "    ft_javadoc = pickle.load(newObj)\n",
    "    for item in ft_javadoc:\n",
    "        ft_javadoc_list.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def flatten(string):\n",
    "    string = string.strip()\n",
    "    string = string.replace('\\n',' ')\n",
    "    string = re.sub('\\s+',' ',string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "\n",
    "####### JAVADOC SPLITTING ######\n",
    "\n",
    "random.shuffle(ft_javadoc_list)\n",
    "\n",
    "train_len_javadoc = round(len(ft_javadoc_list) * 0.8)\n",
    "test_len_javadoc = eval_len_javadoc = round(len(ft_javadoc_list) * 0.1)\n",
    "\n",
    "train_javadoc_instances = ft_javadoc_list[0:train_len_javadoc]\n",
    "test_javadoc_instances =  ft_javadoc_list[train_len_javadoc:train_len_javadoc+test_len_javadoc]\n",
    "eval_javadoc_instances =  ft_javadoc_list[train_len_javadoc+test_len_javadoc:]\n",
    "\n",
    "# First round of duplicates dropping\n",
    "train_javadoc_instances_set = set(train_javadoc_instances)\n",
    "test_javadoc_instances_set = set(test_javadoc_instances)\n",
    "eval_javadoc_instances_set = set(eval_javadoc_instances)\n",
    "\n",
    "# assert(len(train_javadoc_instances_set)==train_javadoc_instances)\n",
    "# assert(len(test_javadoc_instances_set) == test_javadoc_instances)\n",
    "# assert(len(eval_javadoc_instances_set) == eval_javadoc_instances)\n",
    "\n",
    "overlapped_1 = train_javadoc_instances_set.intersection(test_javadoc_instances_set)\n",
    "if len(overlapped_1)>0:\n",
    "    print('Find overlapped in train and test, we are going to remove them')\n",
    "    train_javadoc_instances_set.difference(test_javadoc_instances_set)\n",
    "\n",
    "overlapped_2 = train_javadoc_instances_set.intersection(eval_javadoc_instances_set)\n",
    "if len(overlapped_2 )> 0:\n",
    "    print('Find overlapped in train and eval, we are going to remove them')\n",
    "    train_javadoc_instances_set.difference(eval_javadoc_instances_set)\n",
    "\n",
    "overlapped_3 = test_javadoc_instances_set.intersection(eval_javadoc_instances_set)\n",
    "if len(overlapped_3 )> 0:\n",
    "    print('Find overlapped in test and eval, we are going to remove them')\n",
    "    train_javadoc_instances_set.difference(eval_javadoc_instances_set)\n",
    "\n",
    "train_javadoc_instances = list(train_javadoc_instances_set)\n",
    "test_javadoc_instances = list(test_javadoc_instances_set)\n",
    "eval_javadoc_instances = list(eval_javadoc_instances_set)\n",
    "\n",
    "with open('javadoc_train_instances.pickle', 'wb') as fp:\n",
    "    pickle.dump(train_javadoc_instances, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('javadoc_test_instances.pickle', 'wb') as fp:\n",
    "    pickle.dump(test_javadoc_instances, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('javadoc_eval_instances.pickle', 'wb') as fp:\n",
    "    pickle.dump(eval_javadoc_instances, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "####### SINGLE COMMENT SPLITTING ######\n",
    "\n",
    "random.shuffle(ft_not_keep_comment)\n",
    "\n",
    "train_len_single_comment = round(len(ft_not_keep_comment) * 0.8)\n",
    "test_len_single_comment = eval_len_single_comment = round(len(ft_not_keep_comment) * 0.1)\n",
    "\n",
    "train_single_comment_instances = ft_not_keep_comment[0:train_len_single_comment]\n",
    "test_single_comment_instances =  ft_not_keep_comment[train_len_single_comment:train_len_single_comment+test_len_single_comment]\n",
    "eval_single_comment_instances =  ft_not_keep_comment[train_len_single_comment+test_len_single_comment:]\n",
    "\n",
    "train_single_comment_instances_set = set(train_single_comment_instances)\n",
    "test_single_comment_instances_set  = set(test_single_comment_instances)\n",
    "eval_single_comment_instances_set  = set(eval_single_comment_instances)\n",
    "\n",
    "# assert( len(train_single_comment_instances_set) == train_single_comment_instances)\n",
    "# assert( len(test_single_comment_instances_set)  == test_single_comment_instances)\n",
    "# assert( len(eval_single_comment_instances_set)  == eval_single_comment_instances)\n",
    "\n",
    "overlapped_1 = train_single_comment_instances_set.intersection(test_single_comment_instances)\n",
    "if len(overlapped_1)>0:\n",
    "    print('Find overlapped in train and test, we are going to remove them')\n",
    "    train_single_comment_instances_set.difference(test_single_comment_instances)\n",
    "\n",
    "overlapped_2 = train_single_comment_instances_set.intersection(eval_single_comment_instances_set)\n",
    "if len(overlapped_2 )> 0:\n",
    "    print('Find overlapped in train and eval, we are going to remove them')\n",
    "    train_single_comment_instances_set.difference(eval_single_comment_instances_set)\n",
    "\n",
    "overlapped_3 = test_single_comment_instances_set.intersection(eval_single_comment_instances_set)\n",
    "if len(overlapped_3 )> 0:\n",
    "    print('Find overlapped in test and eval, we are going to remove them')\n",
    "    test_single_comment_instances_set.difference(eval_single_comment_instances_set)\n",
    "\n",
    "train_single_comment_instances = list(train_single_comment_instances_set)\n",
    "test_single_comment_instances = list(test_single_comment_instances_set)\n",
    "eval_single_comment_instances = list(eval_single_comment_instances_set)\n",
    "\n",
    "with open('single_comment_train_instances.pickle', 'wb') as fp:\n",
    "    pickle.dump(train_single_comment_instances, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('single_comment_test_instances.pickle', 'wb') as fp:\n",
    "    pickle.dump(test_single_comment_instances, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('single_comment_eval_instances.pickle', 'wb') as fp:\n",
    "    pickle.dump(eval_single_comment_instances, fp, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def maskTokens(text, n_instances=5):\n",
    "\n",
    "    tokens = text.split(' ')\n",
    "    masked_input = []\n",
    "    output = []\n",
    "\n",
    "    if len(tokens)==1:\n",
    "        return None\n",
    "\n",
    "\n",
    "    elif len(tokens)==2:\n",
    "\n",
    "        #Handling @param\n",
    "        if tokens[0] == '@param' or tokens[0]=='@throws' or tokens[0]=='@exception':\n",
    "            return None\n",
    "\n",
    "        else:\n",
    "            masked_input.append(PLACEHOLDER_MASK)\n",
    "            output.append(tokens[1])\n",
    "\n",
    "            return (masked_input, output)\n",
    "\n",
    "    else:\n",
    "\n",
    "        #Handling specifica javatags\n",
    "        if tokens[0].strip() == '@param' or tokens[0].strip() == '@throws' or tokens[0]=='@exception' :\n",
    "            #tokens = tokens[2:]\n",
    "            choices = list(range(2,len(tokens)))\n",
    "\n",
    "        else:\n",
    "            #tokens = tokens[1:]\n",
    "            choices = list(range(1,len(tokens))) #starting from 1 since the model needs at least one token to provide the suggestion\n",
    "\n",
    "        counter_pop = len(choices)\n",
    "        random.shuffle(choices)\n",
    "        counter_instances = 0\n",
    "\n",
    "        while(True):\n",
    "\n",
    "            token_position = choices.pop()\n",
    "\n",
    "            counter_pop -= 1\n",
    "\n",
    "            counter_instances += 1\n",
    "\n",
    "            dirty_input = ' '.join(tokens[0:token_position]) + ' ' + PLACEHOLDER_MASK\n",
    "            masked_input.append(dirty_input)\n",
    "\n",
    "            dirty_output = ' '.join(tokens[token_position:])\n",
    "            output.append(dirty_output)\n",
    "\n",
    "            if (counter_pop == 0) or (counter_instances == n_instances):\n",
    "                break\n",
    "\n",
    "        return (masked_input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def getInstaces4Sample(comment, sample, javadoc=True):\n",
    "\n",
    "    all_samples_X = []\n",
    "    all_samples_Y = []\n",
    "    sentences = []\n",
    "    to_append = []\n",
    "    backup_sample = sample\n",
    "\n",
    "    if javadoc:\n",
    "        comment = re.sub(\"\\s\\s+\" , \" \", comment)\n",
    "\n",
    "        javadoc_splitted_by_tag = splitJavaDocByTag(comment)\n",
    "\n",
    "        for tag in javadoc_splitted_by_tag:\n",
    "            to_append.append(tag)\n",
    "            comment = comment.replace(tag,'')\n",
    "\n",
    "        i_list = sent_tokenize(comment)\n",
    "\n",
    "        for item in i_list:\n",
    "            sentences.append(item.strip())\n",
    "\n",
    "        for item in to_append:\n",
    "            sentences.append(item)\n",
    "\n",
    "        comment = ' '.join(sentences)\n",
    "\n",
    "        comment = re.sub('\\s+',' ',comment)\n",
    "\n",
    "    else:\n",
    "        sentences = sent_tokenize(comment)\n",
    "\n",
    "    make_comment = ''\n",
    "\n",
    "    for (idx, sent) in enumerate(sentences):\n",
    "\n",
    "        sent = sent.strip()\n",
    "\n",
    "        result = maskTokens(sent)\n",
    "\n",
    "        if result == None: continue\n",
    "\n",
    "        for (item_x, item_y) in zip(result[0],result[1]):\n",
    "\n",
    "            if javadoc: to_replace = (' '+ make_comment + ' ' + item_x + ' ').replace('\\n','')#.strip()\n",
    "            else: to_replace = (' '+ make_comment + ' ' + item_x + ' ').replace('\\n','')\n",
    "\n",
    "            to_replace = re.sub(\"\\s\\s+\" , \" \", to_replace)\n",
    "            sample = sample.replace(comment, to_replace)\n",
    "\n",
    "            all_samples_X.append(sample)\n",
    "            all_samples_Y.append(item_y.replace('\\n',''))\n",
    "            sample = backup_sample\n",
    "\n",
    "        make_comment += sent + ' '\n",
    "\n",
    "    return ( all_samples_X, all_samples_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Since the cleaning process has been performed by taking into account multiple scenarios,\n",
    "# we cannot be totally sure about the validity of a resulting sample. (corner cases management is extremely challenging)\n",
    "# Thereby, this function is the last step before getting the final version of the dataset\n",
    "\n",
    "def checkSampleSanity(sample):\n",
    "    if sample.count('<sep>')  % 2 == 0 and len(sample)>= 3: return 1\n",
    "    else: return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#threshold  = 10000\n",
    "\n",
    "# Loop over these 3 lists to create the final datasets version\n",
    "#ft_javadoc_list\n",
    "#ft_keep_comment\n",
    "#ft_not_keep_comment\n",
    "\n",
    "def createSetBySample(task, split='train'):\n",
    "\n",
    "    #Key: task_name + idx\n",
    "    task_dictionary = {}\n",
    "    all_samples_X = []\n",
    "    all_samples_Y = []\n",
    "\n",
    "    #Select task list\n",
    "    if task == 'javadoc':\n",
    "\n",
    "        if split=='train':\n",
    "            task_list = train_javadoc_instances\n",
    "        elif split == 'eval':\n",
    "            task_list = eval_javadoc_instances\n",
    "        else:\n",
    "            task_list = test_javadoc_instances\n",
    "\n",
    "        flag_javadoc = True\n",
    "\n",
    "    elif task == 'multi_comment':\n",
    "\n",
    "        if split == 'train':\n",
    "            task_list = train_multi_comment_instances\n",
    "\n",
    "        elif split == 'eval':\n",
    "            task_list = eval_multi_comment_instances\n",
    "\n",
    "        else:\n",
    "            task_list = test_multi_comment_instances\n",
    "\n",
    "        flag_javadoc = False\n",
    "\n",
    "    else:\n",
    "\n",
    "        if split == 'train':\n",
    "            task_list = train_single_comment_instances\n",
    "\n",
    "        elif split == 'eval':\n",
    "            task_list = eval_single_comment_instances\n",
    "\n",
    "        else:\n",
    "            task_list = test_single_comment_instances\n",
    "\n",
    "        flag_javadoc = False\n",
    "\n",
    "    for (idx,sample) in enumerate(tqdm(task_list)):\n",
    "\n",
    "        sample = sample.replace('{@link _REF_}',FAKE_REF)\n",
    "        sample = sample.replace('{@link _LINK_}',FAKE_LINK)\n",
    "\n",
    "        if task == 'javadoc':\n",
    "            comments = re.findall(\"<sep>([\\s\\S]*?)<sep>\", sample)\n",
    "            comments = [comments[-1]]\n",
    "        else:\n",
    "            comments = re.findall(\"<sep>([\\s\\S]*?)<sep>\", sample)\n",
    "\n",
    "        if len(comments)>0:\n",
    "\n",
    "           for comment in comments:\n",
    "\n",
    "                x,y = getInstaces4Sample(comment, sample, javadoc=flag_javadoc)\n",
    "\n",
    "                if x == None: continue\n",
    "\n",
    "                for input,label in zip(x,y):\n",
    "\n",
    "                    if '{@link _LINK_}' in input:\n",
    "                        print(input)\n",
    "\n",
    "                    input = input.replace(FAKE_LINK,'{@link _LINK_}')\n",
    "                    input = input.replace(FAKE_REF,'{@link _REF_}')\n",
    "\n",
    "                    if FAKE_REF in input:\n",
    "                        print(label)\n",
    "\n",
    "                    label = label.replace(FAKE_LINK,'{@link _LINK_}')\n",
    "                    label = label.replace(FAKE_REF,'{@link _REF_}')\n",
    "\n",
    "                    all_samples_X.append(input)\n",
    "                    all_samples_Y.append(label)\n",
    "\n",
    "\n",
    "                if task == 'javadoc':\n",
    "                    task_dictionary['javadoc_%s' % idx] = {'input':all_samples_X, 'output':all_samples_Y}\n",
    "                elif task == 'multi_comment':\n",
    "                    task_dictionary['multi_comment_%s' % idx] = {'input':all_samples_X, 'output':all_samples_Y}\n",
    "                else:\n",
    "                    task_dictionary['single_comment_%s' % idx] = {'input':all_samples_X, 'output':all_samples_Y}\n",
    "\n",
    "                all_samples_X = []\n",
    "                all_samples_Y = []\n",
    "\n",
    "    return task_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ft_javadoc_list_dict_train = createSetBySample('javadoc', split='train')\n",
    "ft_javadoc_list_dict_test = createSetBySample('javadoc',  split='test')\n",
    "ft_javadoc_list_dict_eval = createSetBySample('javadoc',  split='eval')\n",
    "\n",
    "ft_single_comment_list_dict_train = createSetBySample('single_comment', split='train')\n",
    "ft_single_comment_list_dict_test = createSetBySample('single_comment', split='test')\n",
    "ft_single_comment_list_dict_eval = createSetBySample('single_comment', split='eval')\n",
    "\n",
    "ft_multi_task_dict_train = {**ft_javadoc_list_dict_train, **ft_single_comment_list_dict_train}\n",
    "ft_multi_task_dict_test = {**ft_javadoc_list_dict_test, **ft_single_comment_list_dict_test}\n",
    "ft_multi_task_dict_eval = {**ft_javadoc_list_dict_eval, **ft_single_comment_list_dict_eval}\n",
    "\n",
    "assert(len(ft_multi_task_dict_train) == len(ft_javadoc_list_dict_train)  + len(ft_single_comment_list_dict_train))\n",
    "assert(len(ft_multi_task_dict_test) == len(ft_javadoc_list_dict_test)  + len(ft_single_comment_list_dict_test))\n",
    "assert(len(ft_multi_task_dict_eval) == len(ft_javadoc_list_dict_eval)  + len(ft_single_comment_list_dict_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ft_javadoc_list_dict = {'train':ft_javadoc_list_dict_train, 'test':ft_javadoc_list_dict_test, 'eval':ft_javadoc_list_dict_eval}\n",
    "ft_single_comment_list_dict = {'train':ft_single_comment_list_dict_train, 'test':ft_single_comment_list_dict_test, 'eval':ft_single_comment_list_dict_eval}\n",
    "ft_multi_task_dict = {'train':ft_multi_task_dict_train, 'test':ft_multi_task_dict_test, 'eval':ft_multi_task_dict_eval}\n",
    "\n",
    "#Saving task-filtered instances\n",
    "with open('javadoc_task_instances.pickle', 'wb') as fp:\n",
    "    pickle.dump(ft_javadoc_list_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('single_comment_task_instances.pickle', 'wb') as fp:\n",
    "    pickle.dump(ft_single_comment_list_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This function creates the multitask dataset for the finetuning\n",
    "def writeFlattenedDataset():\n",
    "\n",
    "    ###### TRAINING SET ######\n",
    "    flatten_input = []\n",
    "    flatten_output = []\n",
    "\n",
    "    for key in ft_multi_task_dict_train.keys():\n",
    "\n",
    "        for (input, output)  in zip(ft_multi_task_dict_train[key]['input'], ft_multi_task_dict_train[key]['output']):\n",
    "\n",
    "            if 'javadoc' in key: target_prefix = JAVADOC_PREFIX\n",
    "            else: target_prefix = BLOCK_COMMENT_PREFIX\n",
    "\n",
    "            flatten_input.append(target_prefix + flatten(input)+ T5_EOS + '\\n')\n",
    "            flatten_output.append(flatten(output)+ T5_EOS + '\\n')\n",
    "\n",
    "    df_train = pd.DataFrame(list(zip(flatten_input, flatten_output)), columns = ['input' , 'output'])\n",
    "    df_train = df_train.sample(frac = 1, random_state=SEED)\n",
    "    train_source = open('train.source','a+')\n",
    "    train_target = open('train.target','a+')\n",
    "\n",
    "    for (input, output) in zip(df_train['input'],df_train['output']):\n",
    "        train_source.write(input)\n",
    "        train_target.write(output)\n",
    "\n",
    "    train_source.close()\n",
    "    train_target.close()\n",
    "\n",
    "    ########################\n",
    "\n",
    "\n",
    "    ###### EVALUATION SET ######\n",
    "    flatten_input = []\n",
    "    flatten_output = []\n",
    "\n",
    "    for key in ft_multi_task_dict_eval.keys():\n",
    "\n",
    "        for (input, output)  in zip(ft_multi_task_dict_eval[key]['input'], ft_multi_task_dict_eval[key]['output']):\n",
    "\n",
    "            if 'javadoc' in key: target_prefix = JAVADOC_PREFIX\n",
    "            else: target_prefix = BLOCK_COMMENT_PREFIX\n",
    "\n",
    "            flatten_input.append(target_prefix + flatten(input)+ T5_EOS + '\\n')\n",
    "            flatten_output.append(flatten(output)+ T5_EOS + '\\n')\n",
    "\n",
    "    df_eval = pd.DataFrame(list(zip(flatten_input, flatten_output)), columns = ['input' , 'output'])\n",
    "    df_eval = df_eval.sample(frac = 1, random_state=SEED)\n",
    "\n",
    "    eval_source = open('eval.source','a+')\n",
    "    eval_target = open('eval.target','a+')\n",
    "\n",
    "    for (input, output) in zip(df_eval['input'],df_eval['output']):\n",
    "        eval_source.write(input)\n",
    "        eval_target.write(output)\n",
    "\n",
    "    eval_source.close()\n",
    "    eval_target.close()\n",
    "\n",
    "    ########################\n",
    "\n",
    "    ###### TEST SET ######\n",
    "    flatten_input = []\n",
    "    flatten_output = []\n",
    "\n",
    "    for key in ft_multi_task_dict_test.keys():\n",
    "\n",
    "        for (input, output)  in zip(ft_multi_task_dict_test[key]['input'], ft_multi_task_dict_test[key]['output']):\n",
    "\n",
    "            if 'javadoc' in key: target_prefix = JAVADOC_PREFIX\n",
    "            else: target_prefix = BLOCK_COMMENT_PREFIX\n",
    "\n",
    "            flatten_input.append(target_prefix + flatten(input)+ T5_EOS + '\\n')\n",
    "            flatten_output.append(flatten(output)+ T5_EOS + '\\n')\n",
    "\n",
    "    df_test = pd.DataFrame(list(zip(flatten_input, flatten_output)), columns = ['input' , 'output'])\n",
    "    df_test = df_test.sample(frac = 1, random_state=SEED)\n",
    "\n",
    "    test_source = open('test.source','a+')\n",
    "    test_target = open('test.target','a+')\n",
    "\n",
    "    for (input, output) in zip(df_test['input'],df_test['output']):\n",
    "        test_source.write(input)\n",
    "        test_target.write(output)\n",
    "\n",
    "    test_source.close()\n",
    "    test_target.close()\n",
    "\n",
    "    ########################\n",
    "\n",
    "writeFlattenedDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def checkForFakeJavadoc(item):\n",
    "    for forbidden in stop_word_java_tag:\n",
    "        if forbidden in item:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#### Final check, since we may have still have some duplicates\n",
    "#### Going to apply set() on the flattened version of the instances\n",
    "\n",
    "####### DUPLICATES CHECK STARTS HERE #########\n",
    "\n",
    "eval_list_input = []\n",
    "eval_list_output = []\n",
    "\n",
    "with open('eval.source') as fread:\n",
    "\n",
    "    for item in fread.readlines():\n",
    "        item = item.strip()\n",
    "        eval_list_input.append(item)\n",
    "\n",
    "\n",
    "with open('eval.target') as fread:\n",
    "\n",
    "    for item in fread.readlines():\n",
    "        item = item.strip()\n",
    "        eval_list_output.append(item)\n",
    "\n",
    "assert(len(eval_list_output) == len(eval_list_input))\n",
    "\n",
    "train_list_input = []\n",
    "train_list_output = []\n",
    "\n",
    "with open('train.source') as fread:\n",
    "\n",
    "    for item in fread.readlines():\n",
    "        item = item.strip()\n",
    "        train_list_input.append(item)\n",
    "\n",
    "\n",
    "with open('train.target') as fread:\n",
    "\n",
    "    for item in fread.readlines():\n",
    "        item = item.strip()\n",
    "        train_list_output.append(item)\n",
    "\n",
    "test_list_input = []\n",
    "test_list_output = []\n",
    "\n",
    "with open('test.source') as fread:\n",
    "\n",
    "    for item in fread.readlines():\n",
    "        item = item.strip()\n",
    "        test_list_input.append(item)\n",
    "\n",
    "with open('test.target') as fread:\n",
    "\n",
    "    for item in fread.readlines():\n",
    "        item = item.strip()\n",
    "        test_list_output.append(item)\n",
    "\n",
    "\n",
    "joined_train = []\n",
    "for item1,item2 in zip(train_list_input, train_list_output):\n",
    "    item1 = item1.strip()\n",
    "    item2 = item2.strip()\n",
    "    new_instance = '{}<COLLEGAMENTO>{}'.format(item1, item2)\n",
    "    joined_train.append(new_instance)\n",
    "\n",
    "joined_test = []\n",
    "for item1,item2 in zip(test_list_input, test_list_output):\n",
    "    item1 = item1.strip()\n",
    "    item2 = item2.strip()\n",
    "    new_instance = '{}<COLLEGAMENTO>{}'.format(item1, item2)\n",
    "    joined_test.append(new_instance)\n",
    "\n",
    "joined_eval = []\n",
    "for item1,item2 in zip(eval_list_input, eval_list_output):\n",
    "    item1 = item1.strip()\n",
    "    item2 = item2.strip()\n",
    "    new_instance = '{}<COLLEGAMENTO>{}'.format(item1, item2)\n",
    "    joined_eval.append(new_instance)\n",
    "\n",
    "set_train = set(joined_train)\n",
    "set_test = set(joined_test)\n",
    "set_eval = set(joined_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "final_train_input = open('train_new.source','a+')\n",
    "final_train_output = open('train_new.target','a+')\n",
    "\n",
    "final_test_input = open('test_new.source','a+')\n",
    "final_test_output = open('test_new.target','a+')\n",
    "\n",
    "overlapped_train_to_test = len(set_train.difference(set_test))\n",
    "no_dup_train = list(set_train.difference(set_test))\n",
    "\n",
    "set_train = set(no_dup_train)\n",
    "overlapped_train_to_eval = len(set_train.difference(set_eval))\n",
    "no_dup_train = list(set_train.difference(set_eval))\n",
    "\n",
    "if len(no_dup_train) < len(set_train):\n",
    "    for item in no_dup_train:\n",
    "        input_x = item.split('<COLLEGAMENTO>')[0]\n",
    "        label =  item.split('<COLLEGAMENTO>')[1]\n",
    "        final_train_input.write(input_x+'\\n')\n",
    "        final_train_output.write(label+'\\n')\n",
    "\n",
    "overlapped_test_eval = len(set_test.difference(set_eval))\n",
    "no_dup_test = list(set_test.difference(set_eval))\n",
    "\n",
    "if len(no_dup_test) < len(set_test):\n",
    "    for item in no_dup_test:\n",
    "        input_x = item.split('<COLLEGAMENTO>')[0]\n",
    "        label =  item.split('<COLLEGAMENTO>')[1]\n",
    "        final_test_input.write(input_x+'\\n')\n",
    "        final_test_output.write(label+'\\n')\n",
    "\n",
    "\n",
    "final_test_output.close()\n",
    "final_test_input.close()\n",
    "final_train_input.close()\n",
    "final_train_output.close()\n",
    "\n",
    "train_set = set(no_dup_train)\n",
    "test_set = set(no_dup_test)\n",
    "duplicated = train_set.intersection(test_set)\n",
    "\n",
    "assert(len(duplicated) == 0)\n",
    "\n",
    "######### DUPLICATES CHECK ENDS HERE #########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# HERE WE'RE GOING TO FURTHER DISCARD BROKEN INSTANCES THAT MAY BE COLLOCATED INTO THE INSIDE-TASK DATASET\n",
    "# THIS HAPPEN WHEN A JAVA METHOD DEFINES OTHER JAVA METHODS WITHIN ITSELF\n",
    "# SPECIFICALLY, THE CODESEARCHNET DATASET ONLY REPORT THE TOP-LEVEL DOCSTRING, THEREFORE WE HAVE TO MANUALLY MANAGE SUCH CASES\n",
    "\n",
    "train_input = open('train_new.source','r')\n",
    "train_target = open('train_new.target','r')\n",
    "\n",
    "train_input_finale = open('train_final.source','a+')\n",
    "train_label_finale = open('train_final.target','a+')\n",
    "\n",
    "for input,target in zip(train_input.readlines(), train_target.readlines()):\n",
    "\n",
    "    input = input.strip()\n",
    "    target = target.strip()\n",
    "\n",
    "    if 'complete block/inline comment' in input:\n",
    "        if checkForFakeJavadoc(input) and checkForFakeJavadoc(target):\n",
    "            train_input_finale.write(input+'\\n')\n",
    "            train_label_finale.write(target+'\\n')\n",
    "    else:\n",
    "        train_input_finale.write(input+'\\n')\n",
    "        train_label_finale.write(target+'\\n')\n",
    "\n",
    "train_input_finale.close()\n",
    "train_label_finale.close()\n",
    "\n",
    "train_input.close()\n",
    "train_target.close()\n",
    "\n",
    "test_input = open('test_new.source','r')\n",
    "test_target = open('test_new.target','r')\n",
    "\n",
    "test_input_finale = open('test_final.source','a+')\n",
    "test_label_finale = open('test_final.target','a+')\n",
    "\n",
    "for input,target in zip(test_input.readlines(), test_target.readlines()):\n",
    "\n",
    "    input = input.strip()\n",
    "    target = target.strip()\n",
    "\n",
    "    if 'complete block/inline comment' in input:\n",
    "        if checkForFakeJavadoc(input) and checkForFakeJavadoc(target):\n",
    "            test_input_finale.write(input+'\\n')\n",
    "            test_label_finale.write(target+'\\n')\n",
    "    else:\n",
    "        test_input_finale.write(input+'\\n')\n",
    "        test_label_finale.write(target+'\\n')\n",
    "\n",
    "test_input_finale.close()\n",
    "test_label_finale.close()\n",
    "\n",
    "test_input.close()\n",
    "test_target.close()\n",
    "\n",
    "\n",
    "eval_input = open('eval.source','r')\n",
    "eval_target = open('eval.target','r')\n",
    "\n",
    "\n",
    "eval_input_finale = open('eval_final.source','a+')\n",
    "eval_label_finale = open('eval_final.target','a+')\n",
    "\n",
    "for input,target in zip(eval_input.readlines(), eval_target.readlines()):\n",
    "\n",
    "    input = input.strip()\n",
    "    target = target.strip()\n",
    "\n",
    "    if 'complete block/inline comment' in input:\n",
    "        if checkForFakeJavadoc(input) and checkForFakeJavadoc(target):\n",
    "            eval_input_finale.write(input+'\\n')\n",
    "            eval_label_finale.write(target+'\\n')\n",
    "    else:\n",
    "        eval_input_finale.write(input+'\\n')\n",
    "        eval_label_finale.write(target+'\\n')\n",
    "\n",
    "eval_input_finale.close()\n",
    "eval_label_finale.close()\n",
    "\n",
    "eval_input.close()\n",
    "eval_target.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Uncomment this cell only for pre-training dataset only\n",
    "\n",
    "# pretrain_flatten = open('pretrain_dataset.txt','a+')\n",
    "\n",
    "# #Here we use the flatten function to flatten the pretraining set as well\n",
    "# for item  in pretrain:\n",
    "#     pretrain_flatten.write(flatten(item)+'\\n')\n",
    "#\n",
    "#\n",
    "# pretrain_flatten.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
