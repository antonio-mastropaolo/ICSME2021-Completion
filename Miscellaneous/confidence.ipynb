{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install statistics\n",
    "from statistics import mean\n",
    "import os,re, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_file = 'T5/javadoc_predictions'\n",
    "targets_file = 'T5/Score-Analysis/javadoc_300000_predictions.targets'\n",
    "scores_file = 'T5/Score-Analysis/javadoc_300000_predictions.scores'\n",
    "\n",
    "with open(predictions_file) as fread:\n",
    "    predictions = [item.strip().strip('\\n') for item in fread.readlines()]\n",
    "\n",
    "with open(targets_file) as fread:\n",
    "    targets = [item.strip().strip('\\n') for item in fread.readlines()]\n",
    "\n",
    "# uncomment for the n-grams\n",
    "# with open(scores_file) as fread:\n",
    "#     scores = [ re.findall('<prob>(.*?)</prob>', item.strip().strip('\\n')) for item in fread.readlines()] \n",
    "\n",
    "with open(scores_file) as fread:\n",
    "    scores = [item.strip().strip('\\n') for item in fread.readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_perfect = dict()\n",
    "confidence_wrong = dict()\n",
    "\n",
    "\n",
    "for i in range(1,16):\n",
    "    confidence_perfect[i] = []\n",
    "    confidence_wrong[i] = []\n",
    "\n",
    "    \n",
    "for token_len in range(1,16):\n",
    "    \n",
    "    \n",
    "    # keep it commented for the 5-gram\n",
    "    scores = [float(s) for s in scores]\n",
    "    \n",
    "    for x, y, z in zip(predictions, targets, scores):\n",
    "        \n",
    "        pred = x.strip().lower().split()\n",
    "        ref = y.strip().lower().split()\n",
    "\n",
    "\n",
    "        if len(ref)>=token_len and len(pred)>=token_len:\n",
    "            \n",
    "            pred = pred[0:token_len]\n",
    "            ref = ref[0:token_len]\n",
    "            \n",
    "            # Get confidence for the n-grams model\n",
    "            #confidence = mean([float(item) for item in z[0:token_len]])\n",
    "            \n",
    "            # Get confidence for the T5-model\n",
    "            confidence = math.exp(z)\n",
    "            \n",
    "            if (''.join(pred) == ''.join(ref)):\n",
    "                confidence_perfect[token_len].append(confidence)\n",
    "            else:\n",
    "                confidence_wrong[token_len].append(confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,16):\n",
    "    \n",
    "    if len(confidence_perfect[i]) > 0 and len(confidence_wrong[i]) > 0:\n",
    "        print('\\n****************************************')\n",
    "        print('Perfect prediction average confidence token len {}: {}'.format(i,mean(confidence_perfect[i])))\n",
    "        print('Wrong prediction average confidence token len {}: {}'.format(i,mean(confidence_wrong[i])))\n",
    "        print('****************************************\\n')\n",
    "    \n",
    "    elif len(confidence_perfect[i]) > 0:\n",
    "        print('\\n****************************************')\n",
    "        print('Perfect prediction average confidence token len {}: {}'.format(i,confidence_perfect[i]))\n",
    "        print('Wrong prediction average confidence token len {}: {}'.format(i,0))\n",
    "        print('****************************************\\n')\n",
    "    \n",
    "    else:\n",
    "        print('\\n****************************************')\n",
    "        print('Perfect prediction average confidence token len {}: {}'.format(i,0))\n",
    "        print('Wrong prediction average confidence token len {}: {}'.format(i,0))\n",
    "        print('****************************************\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
