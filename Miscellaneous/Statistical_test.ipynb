{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def getFileForSTTest(predictions_file_t5, predictions_file_baseline, target_file, dataset='javadoc'):\n",
    "\n",
    "\n",
    "    target = open(target_file,'r')\n",
    "    target_list =  target.readlines()\n",
    "    \n",
    "    baseline_predictions = []\n",
    "    t5_predictions = []\n",
    "    \n",
    "    with open(predictions_file_t5) as fread:\n",
    "        t5_predictions = fread.readlines()\n",
    "        \n",
    "    with open(predictions_file_baseline) as fread:\n",
    "        baseline_predictions = fread.readlines()\n",
    "    \n",
    "    assert(len(target_list) == len(baseline_predictions) == len(t5_predictions))\n",
    "    \n",
    "    \n",
    "    method_id = []\n",
    "    prediction_size = [] \n",
    "    is_perfect_t5 = []\n",
    "    is_perfect_baseline = []\n",
    "    \n",
    "    \n",
    "    for i in range(1,16):\n",
    "\n",
    "        for (id_method,( ref, pred_t5, pred_baseline )) in enumerate(zip(target_list,t5_predictions,baseline_predictions)):\n",
    "\n",
    "            pred_t5 = pred_t5.strip().lower().split()\n",
    "            pred_t5 = pred_t5[0:i]\n",
    "            \n",
    "            pred_baseline = pred_baseline.strip().lower().split()\n",
    "            pred_baseline = pred_baseline[0:i]\n",
    "\n",
    "            ref = ref.strip().lower().split()\n",
    "            ref = ref[0:i]\n",
    "\n",
    "            if len(ref)>=i: #and len(pred)>=i:\n",
    "                \n",
    "                cmp_ref = ''.join(ref)\n",
    "             \n",
    "                if ''.join(pred_t5) == cmp_ref:\n",
    "                    \n",
    "                    if ''.join(pred_baseline) == cmp_ref:\n",
    "                        flag_t5 = True\n",
    "                        flag_baseline = True\n",
    "                    \n",
    "                    else:\n",
    "                        flag_t5 = True\n",
    "                        flag_baseline = False\n",
    "             \n",
    "                elif ''.join(pred_baseline) == cmp_ref:\n",
    "                    flag_t5 = False\n",
    "                    flag_baseline = True\n",
    "                \n",
    "                else:\n",
    "                    flag_t5 = False\n",
    "                    flag_baseline = False\n",
    "                    \n",
    "            idx = '{}_{}_{}'.format(dataset, id_method, i)\n",
    "            method_id.append(idx)\n",
    "            prediction_size.append(i)\n",
    "            is_perfect_t5.append(flag_t5)\n",
    "            is_perfect_baseline.append(flag_baseline)\n",
    "\n",
    " \n",
    "    st_test = pd.DataFrame(zip(method_id, prediction_size, is_perfect_t5, is_perfect_baseline),\n",
    "                           columns=['method_id','prediction_size','is_perfect_T5','is_perfect_baseline'])    \n",
    "    return st_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### EXTRACT FILE FOR STATISTICAL TEST #######\n",
    "import pandas as pd\n",
    "\n",
    "path_to_predictions_jdoc_t5 = ''\n",
    "path_to_predictions_jdoc_baseline = ''\n",
    "path_to_targets_jdoc = ''\n",
    "\n",
    "\n",
    "data = getFileForSTTest(path_to_predictions_jdoc_t5, path_to_predictions_jdoc_baseline, path_to_targets_jdoc, dataset='javadoc')\n",
    "data.to_csv('result_comparison_javadoc.csv',index=False)\n",
    "\n",
    "\n",
    "path_to_predictions_inside_t5 = ''\n",
    "path_to_predictions_inside_baseline = ''\n",
    "path_to_targets_inside = ''\n",
    "\n",
    "\n",
    "data = getFileForSTTest(path_to_predictions_inside_t5, path_to_predictions_inside_baseline, path_to_targets_inside, dataset='inside')\n",
    "data.to_csv('result_comparison_inside.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_javadoc = pd.read_csv('result_comparison_javadoc.csv')\n",
    "df_inside = pd.read_csv('result_comparison_inside.csv')\n",
    "df_overall = pd.concat([df_inside, df_javadoc])\n",
    "\n",
    "assert(len(df_overall) == len(df_javadoc) + len(df_inside))\n",
    "\n",
    "df_overall.to_csv('result_comparison_overall.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/Library/Frameworks/R.framework/Versions/4.0/Resources/bin/Rscript statistical_test.r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
