{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AX3Bdgc9UDs4"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize\n",
    "import sys\n",
    "import string\n",
    "sys.path.append('../../utils/comment_parser/')\n",
    "import comment_parser\n",
    "import random\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from urlextract import URLExtract\n",
    "from codetype import identify\n",
    "import datefinder\n",
    "extractor = URLExtract()\n",
    "\n",
    "PLACEHOLDER = '||_to_remove_||'\n",
    "PLACEHOLDER_LINK = '_LINK_'\n",
    "PLACEHOLDER_REF = '_REF_'\n",
    "PLACEHOLDER_NUM = '_NUM_'\n",
    "FORBIDDEN_TAGS = ['@see', '@version', '@author',  '@since']\n",
    "MAX_LEN = 256\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def allEquals(s):\n",
    "    s = s.strip()\n",
    "    s = s.replace('\\n','')\n",
    "    s = re.sub('\\s+',' ',s)\n",
    "    n = len(s)\n",
    "    for i in range(1, n):\n",
    "        if s[i] != s[0]:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def isAscii(s):\n",
    "    return s.isascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def removeNonAscii(method):\n",
    "\n",
    "    comments = comment_parser.extract_comments_from_str(method, mime='text/x-java-source')\n",
    "\n",
    "    counter = 0\n",
    "    while(True):\n",
    "\n",
    "        if len(comments) == 0:\n",
    "            break\n",
    "\n",
    "        comment_object = comments[counter]\n",
    "        comment = comment_object.text().strip()\n",
    "\n",
    "        if not isAscii(comment):\n",
    "            start = comment_object.indexes()[0]\n",
    "            end = comment_object.indexes()[1]\n",
    "            method = method[0:start] + method[end:]\n",
    "            comments = comment_parser.extract_comments_from_str(method, mime='text/x-java-source')\n",
    "            counter = 0\n",
    "\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter == len(comments):\n",
    "            break\n",
    "\n",
    "    return method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# With the following function we try to manage corner cases like the following one:\n",
    "# //////// comment /////////\n",
    "# k represents the treshold. Can be tuned!\n",
    "def stripChar(comment,k):\n",
    "    target = comment[0:k]\n",
    "    for i in range(len(target)-1):\n",
    "        if target[i] != target[i+1]:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def removeDateTime(method, javadoc):\n",
    "\n",
    "    if javadoc:\n",
    "        try:\n",
    "            matches = list(datefinder.find_dates(method, source=True, index=True))\n",
    "            matches = [match[1] for match in matches if len(match[1].strip())>5 and len(match[1].strip().split(' '))==1] # We're excluding false positive\n",
    "            if len(matches) > 0:\n",
    "                for match in matches:\n",
    "                    method = method.replace(match, PLACEHOLDER_NUM)\n",
    "\n",
    "        except Exception:\n",
    "            return method\n",
    "\n",
    "        return method\n",
    "\n",
    "    else:\n",
    "\n",
    "        comments = re.findall(\"<sep>([\\s\\S]*?)<sep>\", method)\n",
    "        for string in comments:\n",
    "\n",
    "            try:\n",
    "                matches = list(datefinder.find_dates(string, source=True, index=True))\n",
    "                matches = [match[1] for match in matches if len(match[1].strip())>5 and len(match[1].strip().split(' '))==1] # We're excluding false positive\n",
    "\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            if len(matches) > 0:\n",
    "                for match in matches:\n",
    "                    method = method.replace(match, PLACEHOLDER_NUM)\n",
    "\n",
    "\n",
    "        return method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterMultiLineComment(method):\n",
    "\n",
    "    comments = comment_parser.extract_comments_from_str(method, mime='text/x-java-source')\n",
    "\n",
    "    for c in comments:\n",
    "\n",
    "        if c.is_multiline():\n",
    "\n",
    "            c_line = c.line_number()-1\n",
    "\n",
    "\n",
    "            comment = c.text()\n",
    "            comment = removeDateTime(comment, javadoc=False)\n",
    "            comment_lines = comment.splitlines()\n",
    "\n",
    "            final_line = c_line + len(comment_lines)-1\n",
    "\n",
    "            for (idx,line) in enumerate(comment_lines):\n",
    "                if line.strip().startswith('*'):\n",
    "                    comment_lines[idx] = line.replace('*','',1)\n",
    "\n",
    "            comment_refined_lines = '\\n'.join(comment_lines)\n",
    "            replace_this = '<sep> '+comment_refined_lines+' <sep>'\n",
    "            method = method.replace('/*'+comment+'*/',replace_this)\n",
    "            function_lines = method.splitlines()\n",
    "            starting_position_sep = function_lines[c_line].find('<sep>')\n",
    "            ending_position_sep = function_lines[final_line].rfind('<sep>') + 5\n",
    "\n",
    "            if final_line < len(function_lines):\n",
    "                if  function_lines[c_line][0:starting_position_sep].strip() == '' and function_lines[final_line][ending_position_sep:].strip() == '':\n",
    "                    pass\n",
    "\n",
    "                else:\n",
    "                    #Heuristic\n",
    "                    if ( len(function_lines[c_line][0:starting_position_sep].strip()) > 0 and len(function_lines[final_line][ending_position_sep:].strip())==0 ):\n",
    "\n",
    "                        to_replace  = replace_this + '\\n' +function_lines[c_line][0:starting_position_sep]\n",
    "                        to_remove_from_method = function_lines[c_line][0:starting_position_sep]+ replace_this\n",
    "                        method = method.replace(to_remove_from_method, to_replace)\n",
    "\n",
    "                    elif ( len(function_lines[c_line][0:starting_position_sep].strip()) > 0 or len(function_lines[final_line][ending_position_sep:].strip())>0 ):\n",
    "                        method = method.replace(replace_this,'')\n",
    "\n",
    "    return method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Check for SATD\n",
    "def checkSATD(comment):\n",
    "    comment = comment.strip().lower()\n",
    "    return  \\\n",
    "        comment.startswith('to-do') or \\\n",
    "        comment.startswith('todo') or \\\n",
    "        comment.startswith('to_do') or \\\n",
    "        comment.startswith('to-fix')  or \\\n",
    "        comment.startswith('to fix') or \\\n",
    "        comment.startswith('tofix') or \\\n",
    "        comment.startswith('to_fix')  or \\\n",
    "        comment.startswith('fixme') or \\\n",
    "        comment.startswith('fix-me') or \\\n",
    "        comment.startswith('bugfix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function aligns comment with the following format\n",
    "\n",
    "# E.G cdtTrxTxInf.getPmtId().setEndToEndId(SepaUtil.getProperty(sepaParams, SepaUtil.insertIndex))  // comment\n",
    "#\n",
    "# the expected result is:\n",
    "#\n",
    "# // comment\n",
    "# cdtTrxTxInf.getPmtId().setEndToEndId(SepaUtil.getProperty(sepaParams, SepaUtil.insertIndex))\n",
    "\n",
    "#Here the assumption is the following:\n",
    "# if we find the following scenario:\n",
    "# cdtTrxTxInf.getPmtId().setEndToEndId(SepaUtil.getProperty(\n",
    "#                 sepaParams, //c1 c2 c3\n",
    "#                 SepaUtil.insertIndex)) //c2 c3 c4\n",
    "# We assume that c1 c2 c3 are going to be the description of sepaParams and c2 c3 c4 describe sepaUtil.insertIndex\n",
    "\n",
    "def alignComment(method):\n",
    "\n",
    "    comments = comment_parser.extract_comments_from_str(method,mime='text/x-java-source')\n",
    "\n",
    "    # We don't want to handle multiline comments at this level, so we skip here\n",
    "    newlist = [x for x in comments if not x.is_multiline()]\n",
    "\n",
    "    method_lines = method.splitlines()\n",
    "    for c in newlist:\n",
    "\n",
    "        c_line = c.line_number() -1\n",
    "        comment = c.text().strip()\n",
    "        start = len(method_lines[c_line]) - (c.indexes()[1]-c.indexes()[0])\n",
    "        to_replace = method_lines[c_line][0:start] #What comes before the comment (e.g: System.println() // comment)\n",
    "\n",
    "\n",
    "        if method_lines[c_line].replace('//','').strip() == comment:\n",
    "            method_lines[c_line] = '<sep> ' + comment + ' <sep>'\n",
    "        else:\n",
    "            method_lines[c_line] =  '<sep> ' + comment + ' <sep>\\n' + to_replace\n",
    "\n",
    "    ret = keepSepTagAligned('\\n'.join(method_lines))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def maskLinkRef(string, javadoc):\n",
    "\n",
    "    if javadoc:\n",
    "        string = string.replace('\\n',' ')\n",
    "\n",
    "    urls=extractor.find_urls(string)\n",
    "\n",
    "    if len(urls)>0:\n",
    "\n",
    "        for match in urls:\n",
    "            string = string.replace(match, PLACEHOLDER_LINK)\n",
    "\n",
    "    if javadoc:\n",
    "        regex1 = r\"{@link .*?}\"\n",
    "        matches1 = re.findall(regex1, string)\n",
    "\n",
    "        for match in matches1:\n",
    "\n",
    "            if match not in urls:\n",
    "                #reference case\n",
    "                string = string.replace(match, '{@link %s}' % PLACEHOLDER_REF)\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Here we have to pass as parameter only the body of a the method without the javadoc\n",
    "def keepSepTagAligned(method):\n",
    "    target_lines = []\n",
    "    method_lines = method.splitlines()\n",
    "\n",
    "    for (idx, line) in enumerate(method_lines):\n",
    "\n",
    "        if line.strip().startswith('<sep>'):\n",
    "            target_lines.append(idx)\n",
    "\n",
    "    multi_list = []\n",
    "\n",
    "    for k, g in groupby(enumerate(target_lines), lambda ix : ix[0] - ix[1]):\n",
    "       multi_list.append(list(map(itemgetter(1), g)))\n",
    "\n",
    "    for sub_list in multi_list:\n",
    "\n",
    "        if len(sub_list)==1:\n",
    "            continue\n",
    "\n",
    "        if len(sub_list)==2:\n",
    "            method_lines[sub_list[0]] = method_lines[sub_list[0]].rstrip('<sep>')\n",
    "            method_lines[sub_list[1]] = method_lines[sub_list[1]].lstrip('<sep>')\n",
    "\n",
    "        else:\n",
    "            method_lines[sub_list[0]] = method_lines[sub_list[0]].rstrip('<sep>')\n",
    "            for idx_line in sub_list[1:]:\n",
    "                method_lines[idx_line] = method_lines[idx_line].replace('<sep>','')\n",
    "            method_lines[sub_list[-1]] =  method_lines[sub_list[-1]] + ' <sep>'\n",
    "\n",
    "    return '\\n'.join(method_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def removeOrphan(method):\n",
    "\n",
    "    comments = comment_parser.extract_comments_from_str(method, mime='text/x-java-source')\n",
    "\n",
    "    method_lines = method.splitlines()\n",
    "\n",
    "    for comment in comments:\n",
    "\n",
    "        comment_text = comment.text()\n",
    "        c_line = comment.line_number()-1\n",
    "        len_comment_lines = len(comment_text.splitlines())\n",
    "\n",
    "        if c_line > 1:\n",
    "            if len(method_lines[c_line-1].strip())==0 and len(method_lines[c_line+len_comment_lines].strip())==0:\n",
    "                method = method.replace(comment_text, PLACEHOLDER)\n",
    "\n",
    "    return method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def filterCodeAndShort(ref_string, discarded_code_comments=None):\n",
    "\n",
    "    comments = []\n",
    "    forbidden_languages = ['Java', 'JavaScript', 'Objective-C', 'D', 'C++']\n",
    "\n",
    "    prepared_comment = re.findall(\"<sep>([\\s\\S]*?)<sep>\", ref_string)\n",
    "\n",
    "    #Remove short comment\n",
    "    for prep_comment in prepared_comment:\n",
    "        refined_comment = maskLinkRef(prep_comment.strip(), javadoc=False)\n",
    "\n",
    "        if checkSATD(refined_comment) or len(refined_comment.split(' '))<=2:\n",
    "            ref_string = ref_string.replace(prep_comment,PLACEHOLDER)\n",
    "            continue\n",
    "\n",
    "        comments.append(prep_comment)\n",
    "\n",
    "    res = ''\n",
    "    for line in ref_string.splitlines():\n",
    "\n",
    "        if PLACEHOLDER in line:\n",
    "            continue\n",
    "\n",
    "        res+= line + '\\n'\n",
    "\n",
    "    ref_string = res\n",
    "    c_line = 0\n",
    "\n",
    "    for comment in comments:\n",
    "\n",
    "        comment_lines = comment.splitlines()\n",
    "        stripped_comment = comment.strip()\n",
    "        for (idx,line) in enumerate(comment_lines):\n",
    "            if comment in line:\n",
    "                c_line = idx\n",
    "                break\n",
    "\n",
    "        flag_todo = False\n",
    "        for line in comment_lines:\n",
    "            if checkSATD(line):\n",
    "                flag_todo = True\n",
    "                break\n",
    "\n",
    "        if flag_todo:\n",
    "            for index in range(c_line, len(comment_lines) + c_line ):\n",
    "                ref_string = ref_string.replace(comment, '')\n",
    "\n",
    "        #print('iterative check: ',ref_string)\n",
    "\n",
    "        #By putting this check first, we're able to capture corner cases like the following one:\n",
    "        # <sep> ------------System.out.println()-----------<sep>\n",
    "        # In this case we want completely discard such comment from the dataset\n",
    "        if stripped_comment!='' and stripChar(stripped_comment,4):\n",
    "            refined_comment = stripped_comment.strip(stripped_comment[0])\n",
    "\n",
    "            if refined_comment == '':\n",
    "                ref_string = ref_string.replace(comment, '')\n",
    "                continue #skip this comment\n",
    "            else:\n",
    "                ref_string = ref_string.replace(stripped_comment, refined_comment)\n",
    "\n",
    "        # if identify(stripped_comment) == 'AppleScript':\n",
    "        #     print('strange language: ', stripped_comment)\n",
    "\n",
    "        # NB: To filter out commented code we use https://github.com/jdkato/codetype.\n",
    "        # We want to discard as much as possible commented code. Since the dataset we're using is pretty big we do not care of false-positive\n",
    "        if discarded_code_comments:\n",
    "            if identify(stripped_comment) in forbidden_languages:\n",
    "                  discarded_code_comments.write('--> ' + stripped_comment+'\\n')\n",
    "\n",
    "        if identify(stripped_comment) in forbidden_languages or len(stripped_comment.split())<=2:\n",
    "            ref_string = ref_string.replace(stripped_comment, '')\n",
    "            continue\n",
    "\n",
    "    comments = re.findall('<sep>([\\s\\S]*?)<sep>',ref_string)\n",
    "    for item in comments:\n",
    "        #Empty comment after stripping and preprocessing it\n",
    "        if len(item.strip())==0:\n",
    "            ref_string = ref_string.replace('<sep>%s<sep>'% item,'')\n",
    "\n",
    "    return ref_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def checkForMultipleSeparator(method):\n",
    "\n",
    "    lines = method.splitlines()\n",
    "    flag_start = False\n",
    "    starting_line = 0\n",
    "\n",
    "    for (idx,line) in enumerate(lines):\n",
    "\n",
    "        if line.strip().startswith('<sep>') and not flag_start:\n",
    "            starting_line = idx\n",
    "            flag_start = True\n",
    "\n",
    "        if line.strip().endswith('<sep>') and flag_start:\n",
    "            if idx < len(lines):\n",
    "                if lines[idx+1].strip().startswith('<sep>'):\n",
    "                    continue\n",
    "\n",
    "            if starting_line == idx:\n",
    "                flag_start=False\n",
    "                continue\n",
    "\n",
    "\n",
    "            else:\n",
    "                substring = '\\n'.join(lines[starting_line : idx+1])\n",
    "                if substring.count('<sep>') > 2:\n",
    "                    backup = substring\n",
    "                    substring = substring.replace('<sep>','')\n",
    "                    method = method.replace(backup, '<sep> '+substring+' <sep>')\n",
    "                flag_start=False\n",
    "\n",
    "    return method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessInsideComment(string, discarded_code_comments=None):\n",
    "\n",
    "    ref_string = ''\n",
    "\n",
    "    for (idx,line) in enumerate(string.splitlines()):\n",
    "        if line.strip().startswith('@'):\n",
    "            continue\n",
    "        else:\n",
    "            ref_string += line +'\\n'\n",
    "\n",
    "    # Going to filter comments containing non ascii characters\n",
    "    ff = removeNonAscii(ref_string)\n",
    "\n",
    "    # Going to filter orphan comments\n",
    "    ff = removeOrphan(ff)\n",
    "\n",
    "    #print('removeOrphan: ',ff)\n",
    "\n",
    "    # Going to refine multiLineComment\n",
    "    ff = filterMultiLineComment(ff)\n",
    "\n",
    "    # Going to align comment\n",
    "    ff =  alignComment(ff)\n",
    "\n",
    "    #print('align: ', ff)\n",
    "\n",
    "    # Going to check if we can squash subsequent comment into one\n",
    "    result = checkForMultipleSeparator(ff)\n",
    "\n",
    "    # Going to filter out commented code and short comment.\n",
    "    ff = filterCodeAndShort(result, discarded_code_comments)\n",
    "\n",
    "    #print('filterCodeAndShort: ', ff)\n",
    "\n",
    "    # Going to remove dateTime from comments\n",
    "    result = removeDateTime(ff, javadoc=False)\n",
    "\n",
    "    #print('removeDateTime: ',ff)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessDocstring(javadoc):\n",
    "\n",
    "    print('docstring spoca: ',javadoc)\n",
    "    if not isAscii(javadoc):\n",
    "        return 'non-ascii'\n",
    "\n",
    "    #Discard every html tag from the docstring. We use beautiful soup which works like a charm for this kind of job\n",
    "    soup = BeautifulSoup(javadoc)\n",
    "    javadoc=soup.get_text(separator=' ')\n",
    "\n",
    "    lines = javadoc.splitlines()\n",
    "\n",
    "    for (idx,line) in enumerate(lines):\n",
    "        for tag in FORBIDDEN_TAGS:\n",
    "            if tag in line:\n",
    "                lines[idx]=PLACEHOLDER\n",
    "                break\n",
    "\n",
    "    if PLACEHOLDER in '\\n'.join(lines):\n",
    "        javadoc = ''\n",
    "        for line in lines:\n",
    "            if PLACEHOLDER not in line:\n",
    "                javadoc += line + '\\n'\n",
    "\n",
    "    print('dirty docstring: ',javadoc)\n",
    "    docstring = maskLinkRef(javadoc, javadoc=True)\n",
    "    print('cleaned docstring: \"',docstring)\n",
    "    docstring = removeDateTime(docstring, javadoc=True)\n",
    "\n",
    "    docstring_lines = docstring.splitlines()\n",
    "    for line in docstring_lines:\n",
    "        if checkSATD(line.strip()):\n",
    "            return 'invalid'\n",
    "\n",
    "    refinedDocstring = ''\n",
    "    for line in docstring.splitlines():\n",
    "\n",
    "        ############ Starting section ############\n",
    "        if line.startswith('/**'):\n",
    "            line = docstring.lstrip('/**')\n",
    "\n",
    "        if line.startswith('/*'):\n",
    "            line = docstring.lstrip('/*')\n",
    "\n",
    "        if line.endswith('*/'):\n",
    "            line = docstring.rstrip('*/')\n",
    "\n",
    "        ############ ---------------- ############\n",
    "        if line.startswith('*'):\n",
    "            line = docstring.lstrip('*')\n",
    "\n",
    "        refinedDocstring += line + ' '\n",
    "\n",
    "\n",
    "    return refinedDocstring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OeRw85suUDtZ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def preprocessFullDataset(dataset,threshold=0, commented_code_analysis=False, save_pickle=True):\n",
    "\n",
    "    only_docstring_list = []\n",
    "    only_inside_list = []\n",
    "    inside_and_docstring_list = []\n",
    "\n",
    "    if commented_code_analysis: discarded_code_comments = open('discarded_code_comments.txt','a+')\n",
    "    else: discarded_code_comments = None\n",
    "\n",
    "    if threshold>0:\n",
    "        flag_threshold=False\n",
    "    else:\n",
    "        threshold = len(dataset)\n",
    "        flag_threshold=True\n",
    "\n",
    "    # tqdm._instances.clear()\n",
    "\n",
    "    indice = 0\n",
    "\n",
    "    for (idx,item) in enumerate(tqdm(dataset)):\n",
    "\n",
    "        function = item['function']\n",
    "        docstring = item['docstring']\n",
    "\n",
    "        if idx==threshold and not flag_threshold:\n",
    "            return only_docstring_list, only_inside_list, inside_and_docstring_list\n",
    "\n",
    "        #NB: Apparently nltk is much faster than spacy to tokenize a string\n",
    "        list_doc_function = word_tokenize(function)\n",
    "\n",
    "        list_doc_docstring = word_tokenize(docstring)\n",
    "\n",
    "        flag_docstring = True #Assume that each method has its own docstring/javadoc\n",
    "        flag_inline = True #Assume that each method has at least one inline/multiline comment in its own body\n",
    "\n",
    "        #NB: We're counting the #of tokens starting from the 'un-refined' item\n",
    "        if ( len(list_doc_function) + len(list_doc_docstring) ) <= MAX_LEN:\n",
    "\n",
    "            refinedDocstring = preprocessDocstring(docstring)\n",
    "\n",
    "            #Handling function with no comment\n",
    "            try:\n",
    "                refinedFunction = preprocessInsideComment(function, discarded_code_comments)\n",
    "            except Exception:\n",
    "                print('Error here!: ',refinedFunction)\n",
    "                continue\n",
    "\n",
    "            # If we find a non ascii character inside the method we skip it\n",
    "            if not isAscii(refinedFunction):\n",
    "                continue\n",
    "\n",
    "            if refinedFunction.count('<sep>')==0:\n",
    "                flag_inline = False\n",
    "                if (refinedDocstring=='non-ascii' or refinedDocstring=='invalid'):\n",
    "                    continue\n",
    "\n",
    "            if refinedDocstring == 'non-ascii' or refinedDocstring == 'invalid' or len(refinedDocstring.split(' '))<=2:\n",
    "                flag_docstring = False\n",
    "\n",
    "            if len(refinedDocstring.split(' '))>1 and refinedDocstring != '' and ('(non-javadoc)' not in refinedDocstring.lower()):\n",
    "\n",
    "                javadoc_string = refinedDocstring.strip()\n",
    "                javadoc_string = javadoc_string.replace('\\n','')\n",
    "                javadoc_string = re.sub('\\s+',' ',javadoc_string)\n",
    "\n",
    "                # Check the validity of the docstring after the refinements\n",
    "                if javadoc_string != '':\n",
    "\n",
    "                    if stripChar(javadoc_string,4):\n",
    "\n",
    "                        stripped_ref = re.compile('\\w+').findall(javadoc_string)\n",
    "                        javadoc_string = ' '.join(stripped_ref)\n",
    "\n",
    "                        indice += 1\n",
    "\n",
    "                if javadoc_string == '' and not flag_inline:\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "\n",
    "                    if flag_docstring and not flag_inline and len(javadoc_string.split())>2:\n",
    "\n",
    "                        ds_sample = refinedFunction + '\\n' + '<sep> ' + javadoc_string + ' <sep>'\n",
    "\n",
    "                        if len(word_tokenize(ds_sample)) <= MAX_LEN:\n",
    "                            only_docstring_list.append(ds_sample)\n",
    "\n",
    "                    elif flag_inline and flag_docstring:\n",
    "\n",
    "                        ds_sample = refinedFunction + '\\n' + '<sep> ' + javadoc_string + ' <sep>'\n",
    "\n",
    "                        if len(word_tokenize(ds_sample)) <= MAX_LEN:\n",
    "                            # Check if the length is ok\n",
    "                            if len(javadoc_string.split())>2:\n",
    "                                inside_and_docstring_list.append(ds_sample)\n",
    "                            else:\n",
    "                                only_inside_list.append(refinedFunction)\n",
    "\n",
    "                    elif flag_inline and not flag_docstring:\n",
    "                        if len(word_tokenize(refinedFunction)) <= MAX_LEN:\n",
    "                            only_inside_list.append(refinedFunction)\n",
    "\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "            else:\n",
    "                if flag_inline:\n",
    "                    only_inside_list.append(refinedFunction)\n",
    "\n",
    "    if commented_code_analysis:\n",
    "        discarded_code_comments.close()\n",
    "\n",
    "    #First pass of duplicates dropping\n",
    "    only_inside_list = list(dict.fromkeys(only_inside_list))\n",
    "    only_docstring_list = list(dict.fromkeys(only_docstring_list))\n",
    "    inside_and_docstring_list = list(dict.fromkeys(inside_and_docstring_list))\n",
    "\n",
    "    if save_pickle:\n",
    "        with open('only_docstring_list', 'wb') as fp:\n",
    "            pickle.dump(only_docstring_list, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        with open('only_inside_list.pickle', 'wb') as fp:\n",
    "            pickle.dump(only_inside_list, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        with open('inside_and_docstring_list.pickle', 'wb') as fp:\n",
    "            pickle.dump(inside_and_docstring_list, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return only_docstring_list, only_inside_list, inside_and_docstring_list\n",
    "\n",
    "\n",
    "# with open('../../data/raw/java_dedupe_definitions_v2.pkl', 'rb') as f:\n",
    "#     dataset = pickle.load(f, encoding='utf-8')\n",
    "\n",
    "# Is going to take a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Snippet identification business starts here!\n",
    "# It extracts multiple instances\n",
    "def extractSamples(item, keep_comment, withJavaDoc=False):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    comments = re.findall(\"<sep>([\\s\\S]*?)<sep>\", item)\n",
    "\n",
    "\n",
    "    if withJavaDoc:\n",
    "        comments = comments[0:-1]\n",
    "\n",
    "    #print(comments)\n",
    "\n",
    "    splitted_lines = item.splitlines()\n",
    "\n",
    "    for comment in comments:\n",
    "\n",
    "        matching_line = comment.splitlines()[0]\n",
    "\n",
    "        for (matching_index, line) in enumerate(splitted_lines):\n",
    "\n",
    "            if matching_line in line:\n",
    "\n",
    "                comment_lines = len(comment.splitlines())\n",
    "\n",
    "                #forward pass\n",
    "\n",
    "                # Set the upper bound\n",
    "                back_index = matching_index -1\n",
    "                lines_from_begin = splitted_lines[0:back_index]\n",
    "                reversed_list = lines_from_begin[::-1]\n",
    "\n",
    "                flag_exit = False\n",
    "                for (idx, line_back) in enumerate(reversed_list):\n",
    "\n",
    "                    if not keep_comment:\n",
    "\n",
    "                        if '<sep>' in line_back:\n",
    "\n",
    "                            back_index = back_index - idx\n",
    "                            #print('back_index ', splitted_lines[back_index])\n",
    "                            #print('idx: ',idx)\n",
    "                            flag_exit = True\n",
    "                            break\n",
    "\n",
    "                        else:\n",
    "                            pass\n",
    "\n",
    "                    if len(line_back.strip())==0:\n",
    "                        back_index = back_index - idx\n",
    "                        flag_exit = True\n",
    "                        break\n",
    "\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                # In this case the method signature is gonna be the upper bound\n",
    "                if not flag_exit:\n",
    "                    back_index = len(reversed_list)\n",
    "\n",
    "\n",
    "                # Set the lower bound\n",
    "                final_index = matching_index+1\n",
    "\n",
    "                if comment_lines > 1:\n",
    "                    final_index =  (matching_index) + comment_lines\n",
    "                    # print('starting index: ',final_index)\n",
    "                    # print('comment: ', comment)\n",
    "\n",
    "                lines_to_the_end = splitted_lines[final_index:]\n",
    "\n",
    "\n",
    "                flag_exit = False\n",
    "                flag_add_brace = False\n",
    "                for (idx, line_forward) in enumerate(lines_to_the_end):\n",
    "\n",
    "                    if not keep_comment:\n",
    "\n",
    "                        if '<sep>' in line_forward:\n",
    "                            final_index = final_index + idx\n",
    "                            flag_exit = True\n",
    "                            break\n",
    "\n",
    "                        else:\n",
    "                            pass\n",
    "\n",
    "                    if len(line_forward.strip())==0:\n",
    "                        final_index = final_index + idx\n",
    "                        flag_exit = True\n",
    "                        break\n",
    "\n",
    "                    if line_forward.strip().startswith('}'):\n",
    "                        final_index = final_index + idx\n",
    "                        flag_exit = True\n",
    "                        flag_add_brace = True\n",
    "                        break\n",
    "\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                # Push down to the end of the method\n",
    "                if not flag_exit:\n",
    "                    final_index = final_index+len(lines_to_the_end)\n",
    "\n",
    "                sample = '\\n'.join(splitted_lines[back_index:final_index])\n",
    "\n",
    "                if flag_add_brace:\n",
    "                    sample += '\\n}'\n",
    "\n",
    "                # Handling of duplicated instances due to same comments\n",
    "                if sample not in results and sample.count('<sep>')>=2:\n",
    "                    results.append(sample.strip())\n",
    "                    break\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Takes as input the results preprocessFullDataset\n",
    "def datasetSplittingAndPreparation(only_docstring_list=None, only_inside_list=None, inside_and_docstring_list=None):\n",
    "\n",
    "\n",
    "    if only_docstring_list is None:\n",
    "        with open('only_docstring_list.pickle', 'rb') as fp:\n",
    "            only_docstring_list = pickle.load(fp)\n",
    "\n",
    "    if only_inside_list is None:\n",
    "        with open('only_inside_list.pickle', 'rb') as fp:\n",
    "            only_inside_list = pickle.load(fp)\n",
    "\n",
    "    if inside_and_docstring_list is None:\n",
    "        with open('inside_and_docstring_list.pickle', 'rb') as fp:\n",
    "            inside_and_docstring_list = pickle.load(fp)\n",
    "\n",
    "    pre_training_only_doc = round(len(only_docstring_list) * (2/3))\n",
    "    pre_training_only_inside = round(len(only_inside_list) * (2/3))\n",
    "    pre_training_inside_doc = round(len(inside_and_docstring_list) * (2/3))\n",
    "\n",
    "    pretraining_1 = random.sample(only_docstring_list, k=pre_training_only_doc)\n",
    "    pretraining_2 = random.sample(only_inside_list, k=pre_training_only_inside)\n",
    "    pretraining_3 = random.sample(inside_and_docstring_list, k=pre_training_inside_doc)\n",
    "\n",
    "    pretraining_1_3 = []\n",
    "\n",
    "    for (idx, item) in enumerate(pretraining_1):\n",
    "\n",
    "\n",
    "        comment = re.findall(\"<sep>([\\s\\S]*?)<sep>\", item)[0]\n",
    "        sample = item.replace('<sep>','')\n",
    "        sample = sample.replace(comment,'')\n",
    "        sample = '<sep> ' + comment + ' <sep>\\n' + sample\n",
    "\n",
    "        pretraining_1_3.append(sample)\n",
    "\n",
    "    for (idx,item) in enumerate(pretraining_3):\n",
    "\n",
    "        comment = re.findall(\"<sep>([\\s\\S]*?)<sep>\", item)[-1]\n",
    "        item = item.replace('<sep> '+comment.strip()+' <sep>','')\n",
    "        sample = '<sep> ' + comment.strip() + ' <sep>\\n' + item\n",
    "\n",
    "        pretraining_1_3.append(sample)\n",
    "\n",
    "    set_x1 = set(pretraining_1_3)\n",
    "    set_x2 = set(pretraining_2)\n",
    "\n",
    "    pretrain = set_x1.union(set_x2)\n",
    "\n",
    "    set1_original = set(only_docstring_list)\n",
    "    set2_original = set(only_inside_list)\n",
    "    set3_original = set(inside_and_docstring_list)\n",
    "\n",
    "    set1 = set(pretraining_1)\n",
    "    set2 = set(pretraining_2)\n",
    "    set3 = set(pretraining_3)\n",
    "\n",
    "    finetuning_1 = set1_original.difference(set1)\n",
    "    finetuning_2 = set2_original.difference(set2)\n",
    "    finetuning_3 = set3_original.difference(set3)\n",
    "\n",
    "    ft_union = finetuning_1.union(finetuning_2,finetuning_3)\n",
    "    finetune = set(ft_union)\n",
    "\n",
    "    #Check for duplicates between pre-training and fine-tuning instances\n",
    "    assert(len(pretrain.intersection(finetune))==0)\n",
    "\n",
    "    print('Datasets length: ')\n",
    "    print('Fine tuning: ', len(finetune))\n",
    "    print('Pre-training: ',len(pretrain))\n",
    "\n",
    "    #Saving data\n",
    "    with open('pretrain.pickle', 'wb') as fp:\n",
    "        pickle.dump(pretrain, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # Complete finetuning dataset\n",
    "    with open('finetune.pickle', 'wb') as fp:\n",
    "        pickle.dump(finetune, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open('finetuning_1.pickle', 'wb') as fp:\n",
    "        pickle.dump(finetuning_1, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open('finetuning_2.pickle', 'wb') as fp:\n",
    "        pickle.dump(finetuning_2, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open('finetuning_3.pickle', 'wb') as fp:\n",
    "        pickle.dump(finetuning_3, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# datasetSplittingAndPreparation(only_docstring_list=None, only_inside_list=None, inside_and_docstring_list=None)\n",
    "datasetSplittingAndPreparation(only_docstring_list=only_docstring_list, only_inside_list=only_inside_list, inside_and_docstring_list=inside_and_docstring_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#javadoc=True if want to generate the javadoctask-specific dataset as well\n",
    "def createDatasetPerTask(javadoc=True, keep_comment=False, save4analysis=False):\n",
    "\n",
    "    finetune_inside = []\n",
    "    finetune_docstring = []\n",
    "\n",
    "    # ************** Loading data **************\n",
    "\n",
    "    # Loading Javadoc pickle\n",
    "    if javadoc:\n",
    "        with open('....', 'rb') as finetuning_1:\n",
    "            ft1 = pickle.load(finetuning_1)\n",
    "\n",
    "        for item in list(ft1):\n",
    "            finetune_docstring.append(item)\n",
    "\n",
    "        with open('....', 'rb') as finetuning_3:\n",
    "            ft3 = pickle.load(finetuning_3)\n",
    "\n",
    "    with open('....', 'rb') as finetuning_2:\n",
    "        ft2 = pickle.load(finetuning_2)\n",
    "\n",
    "    # Loading finetuning pickle\n",
    "    for item in list(ft2):\n",
    "\n",
    "        rr = extractSamples(item, keep_comment=keep_comment)\n",
    "\n",
    "        for comment in rr:\n",
    "            finetune_inside.append(comment)\n",
    "\n",
    "    for item in list(ft3):\n",
    "\n",
    "        rr=extractSamples(item, keep_comment=keep_comment, withJavaDoc=True)\n",
    "\n",
    "        for comment in rr:\n",
    "            finetune_inside.append(comment)\n",
    "\n",
    "        if javadoc:\n",
    "            finetune_docstring.append(item)\n",
    "\n",
    "    #Save finetuning for the javadoc\n",
    "    if javadoc:\n",
    "        set_1_ft_docstring = list(dict.fromkeys(finetune_docstring))\n",
    "\n",
    "        with open('finetuning_javadoc.pickle', 'wb') as fp:\n",
    "            pickle.dump(set_1_ft_docstring, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    set_2_ft_inside = list(dict.fromkeys(finetune_inside))\n",
    "\n",
    "    if keep_comment: filename_ft = 'finetuning_multi_comment'\n",
    "    else: filename_ft = 'finetuning_single_comment'\n",
    "\n",
    "    with open('%s.pickle' % filename_ft, 'wb') as fp:\n",
    "        pickle.dump(set_2_ft_inside, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    if save4analysis:\n",
    "\n",
    "        ft_txt = open('%s.txt' % filename_ft,'a+')\n",
    "        for (idx,element) in enumerate(list(set_2_ft_inside)):\n",
    "            ft_txt.write('idx: '+str(idx)+' '+element+'\\n')\n",
    "        ft_txt.close()\n",
    "\n",
    "        if javadoc:\n",
    "\n",
    "            ft_txt = open('finetuning_docstring.txt','a+')\n",
    "            for (idx,element) in enumerate(list(set_1_ft_docstring)):\n",
    "                ft_txt.write('idx: '+str(idx)+' '+element+'\\n')\n",
    "            ft_txt.close()\n",
    "\n",
    "#If keep_comment = True then we expand the context to non target-comment as well\n",
    "createDatasetPerTask(javadoc=True, keep_comment=False, save4analysis=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled-3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PyCharm (Auto_CM_Completion)",
   "language": "python",
   "name": "pycharm-ca6c26a7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
